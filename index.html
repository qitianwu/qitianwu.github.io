<meta name="google-site-verification" content="Y50gVg4R1617ZJjuqs37mnvW9fMjlWxJ0J13I1sULSI" />
<meta name="description" content="Qitian Wu&#39;s home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Qitian Wu's Homepage @ Shanghai Jiao Tong University</title>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?f38d019abe6aa72687acdcf683ca0515";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>

<body>
<div id="layout-content" style="margin-top:25px">

<table>
    <tbody>
        <tr>
            <td width="670">
                <div id="toptitle">
                    <h1>Qitian Wu &nbsp; <!--<img src="images/name.png" height="45px" style="margin-bottom:-5px" alt=''>--></h1>
                </div>
                <p>
                    Shanghai Jiao Tong University <br>
                    Shanghai, China, 200240.<br>
                    Email: echo740@sjtu.edu.cn<br>
                    <!--
                    <a href="assets/cv.pdf">CV</a> |
                    -->
                    <a href="https://github.com/qitianwu">Github</a> | <a href="https://scholar.google.com/citations?user=m01-2qUAAAAJ&hl=zh-CN">Google Scholar</a>
                </p>
            </td>
        </tr>
    </tbody>
</table>
    

<table>
        <tbody>
            <tr>
                <th>
                    <img src="images/self3.jpeg" border="0" width="250">
                </th>
                <td style="width: 5%"></td>
                <td>
                    <p>I am a PhD student at Department of Computer Science and Engineering from Shanghai Jiao Tong University (SJTU), advised by
                        Prof. Junchi Yan. I achieved the Bachelor (Microelectronics, Mathematics) and Master (Computer Science) degrees from SJTU.
                    </p>
                    <p>My general research interests lie in out-of-distribution generalization, structured learning and inference, and generative models, with 
                        their applications in recommender systems, drug discovery and advertisement.</p>
                    <p>My current research focus on building scalable/expressive structured representation models and generalizable/reliable learning algorithms 
                        over semi-structured data (graphs, sequences, user behaviors, etc.). I endeavor on developing the 
                        next generation of learning approaches for dynamic, open and uncertain environments.</p>
                </td>
            </tr>
        </tbody>
    </table>


<h2>Recent News <small></small></h2>
<ul>
    <li>
        [2022.10] I was awarded with National Scholarship.
    </li>
    <li>
        [2022.09] Papers on out-of-distribution generalization/detection, graph structure learning, temporal causal learning and molecular 
        representation were accepted to NeurIPS'22.
    </li>
    <li>
        [2022.05] Papers on weakly-supervised graph neural networks and adversarial robustness were accepted to SIGKDD'22.
    </li>
    <li>
        [2022.04] One paper on negative sampling in recsys was accepted to IJCAI'22.
    </li>
    <li>
        [2022.01] One paper on out-of-distritbuion generalization on graphs was accepted to ICLR'22.
    </li>
    <li>
        [2021.11] I was awarded with Baidu PhD Scholarship.
    </li>
    <li>
        [2021.10] I was awarded with Microsoft Research PhD Fellowship.
    </li>
    <li>
        [2021.09] Papers on feature extrapolation, self-supervised GNNs and deep generative models were accepted to NeurIPS'21.
    </li>
    <li>
        [2021.08] One paper on sequential behavior modeling was accepted to CIKM'21 as spotlight.
    </li>
    <li>
        [2021.04] I was elected as Global Top 100 AI Rising Start by Baidu.
    </li>
</ul>


<h2>Selected Publications (see a complete list in <a href="https://scholar.google.com/citations?user=m01-2qUAAAAJ&hl=zh-CN">Google Scholar</a>) <small></small></h2>
<ul>
    <li>
        <a href="https://www.researchgate.net/publication/363845271_NodeFormer_A_Scalable_Graph_Structure_Learning_Transformer_for_Node_Classification">
            NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification</a>  <br />
        <b>Qitian Wu</b>, Wentao Zhao, Zenan Li, David Wipf and Junchi Yan<br />
        <i>Advances in Neural Information Processing Systems</i> <b>(NeurIPS 2022)</b><br />
        <!-- <ul><li>Keywords: Graph Transformer, Graph Structure Learning, Large-Scale Graph Learning</li></ul> -->
        <ul><li>Summary: We propose a scalable graph structure learning model with efficient all-pair message passing on latent graphs, 
            enabled by a kernelized Gumbel-Softmax operator which has linear complexity w.r.t. node numbers.
        </li></ul>
        <ul><li>Materials:
            <a href="assets/NodeFormer-slides.pdf"> Slides</a> |
            <a href="https://github.com/qitianwu/NodeFormer"> Codes</a>
        </li></ul>
    </li>
    <li>
        <a href="https://arxiv.org/abs/2202.02466">
            Handling Distribution Shifts on Graphs: An Invariance Perspective</a>  <br />
        <b>Qitian Wu</b>, Hengrui Zhang, Junchi Yan and David Wipf<br />
        <i>International Conference on Learning Representations</i> <b>(ICLR 2022)</b><br />
        <!-- <ul><li>Keywords: Graph Neural Networks, Out-of-Distribution Generalization, Domain-Invariant Learning</li></ul> -->
        <ul><li>Summary: We formulate out-of-distribution generalization problem on graphs and propose a provably effective invariant learning approach. 
            As a piorneering work, we systematically discuss how to leverage invariance principle for handling node-level distribution shifts on graphs.
        </li></ul>
        <ul><li>Materials:
            <a href="assets/EERM-slides.pdf"> Slides</a> |
            <a href="assets/EERM-poster.png"> Poster</a> |
            <a href="https://github.com/qitianwu/GraphOOD-EERM"> Codes</a>
        </li></ul>
    </li>
    <li>
        <a href="https://arxiv.org/abs/2110.04514">
            Towards Open-World Feature Extrapolation: An Inductive Graph Learning Approach</a>  <br />
        <b>Qitian Wu</b>, Chenxiao Yang and Junchi Yan<br />
        <i>Advances in Neural Information Processing Systems</i> <b>(NeurIPS 2021)</b><br />
        <!-- <ul><li>Keywords: Attribute Feature Representation, Graph Neural Networks, Extrapolation, Domain Shift</li></ul> -->
        <ul><li>Summary: We study a new problem, i.e., feature space expansion from training data to testing data, and propose a graph-enhanced
            representation learning approach for handling tabular data with variable-length features. 
        </li></ul>
        <ul><li>Materials:
            <a href="assets/FATE-slides.pdf"> Slides</a> |
            <a href="assets/FATE-poster.png"> Poster</a> |
            <a href="https://github.com/qitianwu/FATE"> Codes</a>
        </li></ul>
    </li>
    <li>
        <a href="https://arxiv.org/abs/2007.04833">
            Towards Open-World Recommendation: An Inductive Model-based Collaborative Filtering Approach</a><br />
        <b>Qitian Wu</b>, Hengrui Zhang, Xiaofeng Gao, Junchi Yan and Hongyuan Zha<br />
        <i>In International Conference on Machine Learning</i> <b>(ICML 2021)</b><br />
        <!-- <ul><li>Keywords: Inductive Representation Learning, Cold-start Recommendation/Learning, Matrix Completion, Graph Structure Learning.</li></ul> -->
        <ul><li>Summary: We consider an inductive collaborative filtering setting where testing users are unappeared in the training phase, and propose to leverage 
            representations of existing users to compute the new ones' through latent graph structures. 
        </li></ul>
        <ul><li>Materials:
            <a href="assets/IDCF-slides.pdf"> Slides</a> |
            <a href="assets/IDCF-poster.png"> Poster</a> |
            <a href="https://github.com/qitianwu/IDCF"> Codes</a>
        </li></ul>
    </li>
    <li>
        <a href="https://www.researchgate.net/publication/363672516_Learning_Substructure_Invariance_for_Out-of-Distribution_Molecular_Representations">
            Learning Substructure Invariance for Out-of-Distribution Molecular Representations</a>  <br />
        Nianzu Yang, Kaipeng Zeng, <b>Qitian Wu</b>, Xiaosong Jia and Junchi Yan<br />
        <i>Advances in Neural Information Processing Systems</i> <b>(NeurIPS 2022)</b><br />
        <!-- <ul><li>Keywords: Molecular Representation, Distribution Shifts, Domain-Invariant Learning</li></ul> -->
        <ul><li>Summary: We build a substructure-aware invariant learning approach for out-of-distribution generalization in molecular representation.
            The new method outperforms the strongest baselines on OGB molecular property prediction and DrugOOD benchmarks by 5.9% and 3.9% in terms of ROC-AUC.
        </li></ul>
        <ul><li>Materials:
            <a href="https://github.com/yangnianzu0515/MoleOOD"> Codes</a>
        </li></ul>
    </li>
    <li>
        <a href="https://www.researchgate.net/publication/364290694_GraphDE_A_Generative_Framework_for_Debiased_Learning_and_Out-of-Distribution_Detection_on_Graphs">
            GraphDE: A Generative Framework for Debiased Learning and Out-of-Distribution Detection on Graphs</a>  <br />
        Zenan Li, <b>Qitian Wu</b>, Fan Nie and Junchi Yan<br />
        <i>Advances in Neural Information Processing Systems</i> <b>(NeurIPS 2022)</b><br />
        <ul><li>Keywords: Graph Representation Learning, Out-of-Distribution Detection, Debiased Learning</li></ul>
        <ul><li>Summary: We introduce a probabilistic generative framework for out-of-distribution detection and debiased learning on graph data. 
            The key insight is that the two problems can be unified under a single framework.
        </li></ul>
        <ul><li>Materials:
            <a href="https://github.com/Emiyalzn/GraphDE"> Codes</a>
        </li></ul>
    </li>
    <li>
        <a href="https://arxiv.org/pdf/2210.13005.pdf">
            Towards Out-of-Distribution Sequential Event Prediction: A Causal Treatment</a>  <br />
        Chenxiao Yang, <b>Qitian Wu</b>, Qingsong Wen, Zhiqiang Zhou, Liang Sun and Junchi Yan<br />
        <i>Advances in Neural Information Processing Systems</i> <b>(NeurIPS 2022)</b><br />
        <!-- <ul><li>Keywords: Sequential Recommendation, Temporal Distribution Shifts, Causal Learning</li></ul> -->
        <ul><li>Summary: We use a causal perspective to analyze sequential event prediction under temporal distribution shifts and 
            propose variational context adjustment as an effective treatment.
        </li></ul>
        <ul><li>Materials:
            Coming soon
        </li></ul>
    </li>
    <li>
        <a href="https://arxiv.org/pdf/2210.13014.pdf">
            Geometric Knowledge Distillation: Topology Compression for Graph Neural Networks</a>  <br />
        Chenxiao Yang, <b>Qitian Wu</b> and Junchi Yan<br />
        <i>Advances in Neural Information Processing Systems</i> <b>(NeurIPS 2022)</b><br />
        <!-- <ul><li>Keywords: Graph Neural Networks, Diffusion Process, Knowledge Compression</li></ul> -->
        <ul><li>Summary: We propose a geometric knowledge distillation model that can generalize the topological knowledge from larger GNNs to smaller one.
            The key insight is to capture the underlying geometric diffusion process within GNNs that are invariant to graph sizes.
        </li></ul>
        <ul><li>Materials:
            Coming soon
        </li></ul>
    </li>
    <li>
        <a href="https://dl.acm.org/doi/abs/10.1145/3534678.3539283">
            Variational Inference for Training Graph Neural Networks in Low-Data Regime through Joint Structure-Label Estimation</a> <br />
            Danning Lao, Xinyu Yang, <b>Qitian Wu</b>, Junchi Yan <br />
        <i>ACM SIGKDD Conference on Knowledge Discovery and Data Mining</i> <b>(SIGKDD 2022)</b><br />
        <!-- <ul><li>Keywords: Graph Neural Networks, Weakly-supervised Learning, Graph Structure Inference</li></ul> -->
        <ul><li>Summary: We propose a generative framework for training graph neural networks with incompletely observed data where labels and structures are jointly missing.
            The new approach outperforms GCN by 75.6% when only 1 label per class and 1% edges are observed on Cora.
        </li></ul>
        <ul><li>Materials:
            <a href="https://github.com/Thinklab-SJTU/WSGNN"> Codes</a>
        </li></ul>
    </li>
    <li>
        <a href="https://dl.acm.org/doi/abs/10.1145/3534678.3539242">
            DICE: Domain-attack Invariant Causal Learning for Improved Data Privacy Protection and Adversarial Robustness</a> <br />
            Qibing Ren, Yiting Chen, Yichuan Mo, <b>Qitian Wu</b>, Junchi Yan<br />
        <i>ACM SIGKDD Conference on Knowledge Discovery and Data Mining</i> <b>(SIGKDD 2022)</b><br />
        <!-- <ul><li>Keywords: Domain-Invariant Learning, Adversarial Attack and Robustness, Privacy Protection </li></ul> -->
        <ul><li>Summary: We adopt a data-generative causal perspective to unify the formulation of privacy protection and adversarial robustness 
            from a bottom-up perspective. Based on this, a new principled approach is proposed for performance boosting.
        </li></ul>
        <ul><li>Materials:
            <a href="https://github.com/Thinklab-SJTU/DICE"> Codes</a>
        </li></ul>
    </li>
    
    <li>
        <a href="https://arxiv.org/abs/1909.13035">
            Bridging Explicit and Implicit Deep Generative Models via Neural Stein Estimators</a>  <br />
        <b>Qitian Wu</b>, Han Gao and Hongyuan Zha<br />
        <i>Advances in Neural Information Processing Systems</i> <b>(NeurIPS 2021)</b><br />
        <!-- <ul><li>Keywords: Deep Generative Models, Energy-based Model, Generative Adversarial Networks, Stein's Method</li></ul> -->
        <ul><li>Summary: We leverage the Stein's distance as a novel tool to construct a theoretically grounded bridging term between two families of deep generative models.
        </li></ul>
        <ul><li>Materials:
            <a href="assets/Stein-slides.pdf"> Slides</a> |
            <a href="assets/Stein-poster.png"> Poster</a> |
            <a href="https://github.com/qitianwu/SteinBridging"> Codes</a>
        </li></ul>
    </li>
    <li>
        <a href="https://arxiv.org/abs/2106.12484">
            From Canonical Correlation Analysis to Self-supervised Graph Neural Networks</a><br />
        Hengrui Zhang, <b>Qitian Wu</b>, Junchi Yan, David Wipf and Philip S. Yu<br />
        <i>Advances in Neural Information Processing Systems</i> <b>(NeurIPS 2021)</b><br />
        <!-- <ul><li>Keywords: Graph Neural Networks, Self-supervised Representation Learning, Contrastive Learning</li></ul> -->
        <ul><li>Summary: We introduce a simple yet effective contrastive objective for self-supervised node representation learning over graphs. 
            Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly.
        </li></ul>
        <ul><li>Materials:
            <a href="https://github.com/hengruizhang98/CCA-SSG"> Codes</a>
        </li></ul>
    </li>
    <!-- <li>
        <a href="assets/Seq2Bubble.pdf">
            Seq2Bubbles: Region-Based Embedding Learning for User Behaviors in Sequential Recommenders</a><br />
        <b>Qitian Wu</b>, Chenxiao Yang, Shuodian Yu, Xiaofeng Gao and Guihai Chen<br />
        <i>ACM International Conference on Information & Knowledge Management</i> <b>(CIKM 2021, spotlight)</b><br />
        <ul><li>Keywords: Sequential Recommendation, User Behavior Modeling, Geometrically-inspired Embedding</li></ul>
        <ul><li>Summary: Most of existing neural recommendation models embed a user behavior sequence into a single point in the latent space. To resolve the limited expressivity, we 
            propose to embed a sequence into a set of closed regions (hyper-ellipsoid) where center vectors can model a user's interests of multiple aspects and radius vectors
            could accommodate heterogeneous concentration over distinct aspects.
        </li></ul>
        <ul><li>Materials:
            <a href="assets/Seq2Bubbles-slides.pdf"> Slides</a> |
            <a href="https://github.com/qitianwu/IDCF"> Codes</a>
        </li></ul>
    </li> -->
    <li>
        <a href="assets/LANTERN.pdf">
            Learning Latent Process from High-Dimensional Event Sequences via Efficient Sampling</a><br /> 
        <b>Qitian Wu</b>, Zixuan Zhang, Xiaofeng Gao, Junchi Yan and Guihai Chen<br /> 
        <i>In Advances in Neural Information Processing Systems</i> <b>(NeurIPS 2019)</b><br />
        <!-- <ul><li>Keywords: Temporal Point Process, Relation Inference, Sequence Transformer, Imitation Learning</li></ul> -->
        <ul><li>Summary: We target the modeling and generation of temporal dynamics over a large interaction latent graph, and 
            propose a scalable and efficient sampling approach for extremely high-dimensional sequences.
        </li></ul>
        <ul><li>Materials:
            <a href="assets/LANTERN-poster.pdf"> Poster</a> |
            <a href="https://github.com/zhangzx-sjtu/LANTERN-NeurIPS-2019"> Codes</a>
        </li></ul>
    </li>
    <!-- <li>
        <a href="assets/Feature Evolution.pdf">
            Feature Evolution Based Multi-Task Learning for Collaborative Filtering with Social Trust</a><br /> 
        <b>Qitian Wu</b>, Lei Jiang, Xiaofeng Gao, Xiaochun Yang and Guihai Chen<br /> 
        <i>In International Joint Conference on Artificial Intelligence</i> <b>(IJCAI 2019)</b><br />
        <ul><li>Keywords: Collaborative Filtering, Network Embedding, Multi-Task Learning, Bayesian Optimization.</li></ul>
    </li> -->
    <li>
        <a href="assets/Dual Sequential.pdf">
            Dual Sequential Prediction Models Linking Sequential Recommendation and Information Dissemination</a><br /> 
        <b>Qitian Wu</b>, Yirui Gao, Xiaofeng Gao, Paul Weng and Guihai Chen<br />
        <i>In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i> <b>(KDD 2019)</b><br /> 
        <!-- <ul><li>Keywords: Sequential Recommendation, Information Diffusion, User Behavior Modeling.</li></ul> -->
        <ul><li>Summary: We propose a dual learning model that resorts to mutual reinforcement between two sequential learning models via a teacher-student distillation loss.
        </li></ul>
        <ul><li>Materials:
            <a href="assets/DEEMS-poster.png"> Poster</a> |
            <a href="https://github.com/qitianwu/DEEMS-KDD-19"> Codes</a>
        </li></ul>
        </li>
    <li>
        <a href="assets/Dual Graph.pdf">
            Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems</a><br />       
        <b>Qitian Wu</b>, Hengrui Zhang, Xiaofeng Gao, Peng He, Paul Weng, Han Gao and Guihai Chen<br />
        <i>In World Wide Web Conference</i> <b>(WWW 2019)</b><br />
        <!-- <ul><li>Keywords: Social Recommendation, Graph Neural Networks, Multi-Armed Bandit, Policy Gradient</li></ul> -->
        <ul><li>Summary: We design a dual graph attention architecture
            that simultaneously learns to capture user-user relations and item-item relations in a 
            fully data-driven manner.
        </li></ul>
        <ul><li>Materials:
            <a href="assets/DANSER-slides.png"> Slides</a> |
            <a href="assets/DANSER-poster.png"> Poster</a> |
            <a href="https://github.com/qitianwu/DANSER-WWW-19"> Codes</a>
        </li></ul>
    </li>
    <!-- <li>
        <a href="https://dl.acm.org/doi/abs/10.1145/3269206.3271714">
            Adversarial Training Model Unifying Feature Driven and Point Process Perspectives for Event Popularity Prediction</a><br /> 
        <b>Qitian Wu</b>, Chaoqi Yang, Hengrui Zhang, Xiaofeng Gao, Paul Weng and Guihai Chen<br />
        <i>In ACM International Conference on Information and Knowledge Management</i> (CIKM 2018)</b><br />
        <ul><li>Keywords: Event Prediction, Point Process, Adversarial Training</li></ul>
        <ul><li>Summary: Predicting future events given histortical ones is a fundamental and pervasive problem in machine learning and data mining, with applications
            to social influence analysis, pandemic control and target advertisement. Feature driven methods and point process models are two schools of thinking in this area. 
            In this paper we propose an adversarial training model that combines both of the world through a two-player game.
        </li></ul>
    </li>
    <li>
        <a href="https://ieeexplore.ieee.org/document/8594984">
            EPAB: Early Pattern Aware Bayesian Model for Social Content Popularity Prediction</a><br /> 
        <b>Qitian Wu</b>, Chaoqi Yang, Xiaofeng Gao, Peng He and Guihai Chen<br />
        <i>In IEEE International Conference on Data Mining</i> (ICDM 2018)<br />
        <ul><li>Keywords: Information Diffusion, Bayesian Networks, Structure Searching</li></ul>
        <ul><li>Summary: This works focus on predicting the popularity of user generated content in social networks (e.g., tweets or weibo) and proposes a probabilistic framework based on
            Bayesian networks that is tailored for early-stage popularity prediction where the observed information is rare.
    </li> -->
</ul>


<h2>Selected Awards</h2>
<table style="border-spacing:2px">
    <tbody>
        <ul>
            <li>
                National PhD Scholarship
            </li>
            <li>
                Baidu PhD Fellowship (only 10 from worldwide PhD students)
            </li>
            <li>
                Microsoft Research PhD Fellowship (only 11 in Asia)
            </li>
            <li>
                Baidu Global Top 100 Rising Stars in ArtiÔ¨Åcial Intelligence
            </li>
            <li>
                National Scholarship (2016, 2017, top 1 in the department)
            </li>
            <li>
                The 1st-Class Academic Excellence Scholarship (2016, 2017, top 1 in the department)
            </li>
            <li>
                Lixin Tang Scholarship (2017, 2018)
            </li>
            <li> 
                Yuanqing Yang Scholarship (2019)
            </li>
            <li>
                Outstanding Winner, INFORMS Awards, Mathematical Contest in Modeling, Data Insights Problem
                (top 3 out of 4748 teams, the INFORMS Awards selects one team among all the participants) 
                
                                    <a href="assets/72969.pdf">[paper]</a>
                <ul><li>Summary: We build an interpretable gaussian process regression model for energy consumption prediction and a 
                    principled multi-objective optimization model for social policy improvement.</li></ul>
               
            </li>
            <li>
                National Second Award, China Undergraduate Mathematical Contest in Modeling (top 5.8% in 28046 teams)
            </li>
            <li>
                First Award, Physics Competition of Chinese College Students (2015)
            </li>
            <li>
                Outstanding Graduate of Shanghai (2018)
            </li>
            <li>
                Excellent Thesis of Undergraduates (2018)
            </li>
        </ul>
    </tbody>
</table>


<h2>Academic Service</h2>
<table style="border-spacing:2px">
    <tbody>
            <p>I will serve or served as the reviewer or (senior) PC member for TKDE, TNNLS, AAAI'21, CVPR'21, IJCAI'21, ICML'21, ICCV'21, NeurIPS'21, 
                ICLR'22, AAAI'22, CVPR'22, IJCAI'22, ICML'22, NeurIPS'22, AAAI'23, ICLR'23, LoG'23.</p>
    </tbody>
</table>


<!--
<h2>Activity</h2>
<table>
    <tbody>
        <tr>
            <th>
                <td>
                <img src="images/nips-poster1.jpg" border="0" width="130">
                </td>
                <td style="height: 5%"></td>
                <td>
                <img src="images/nips-poster2.jpg" border="0" width="130">
                </td>
            </th>
            <td style="width: 5%"></td>
            <td>
                <p><b>2019.12:</b> I attended NeurIPS'19 at Vancouver, Canada, and gave a poster presentation for our paper.</p>
            </td>
        </tr>
    </tbody>
</table>
<table>
    <tbody>
        <tr>
            <th>
                <td>
                <img src="images/www-pre.jpg" border="0" width="130">
                </td>
                <td style="height: 5%"></td>
                <td>
                <img src="images/www-poster.jpg" border="0" width="130">
                </td>
            </th>
            <td style="width: 5%"></td>
            <td>
                <p><b>2019.05:</b> I gave an oral presentation about our work in WWW'19 conference at San Francisco, USA.</p>
            </td>
        </tr>
    </tbody>
</table>
<table>
        <tbody>
            <tr>
                <th>
                    <td>
                        <img src="images/icdm-pre.jpg" border="0" width="130">
                        </td>
                        <td style="height: 5%"></td>
                        <td>
                        <img src="images/icdm-poster.jpg" border="0" width="130">
                    </td>
                </th>
                <td style="width: 5%"></td>
                <td>
                    <p><b>2018.11:</b> I attended ICDM'18 conference held at Singapore and gave an presentation about our work.</p>
                </td>
            </tr>
        </tbody>
    </table>

-->


<div id="footer">
    <div id="footer-text"></div>
</div>

</div>

</body></html>

